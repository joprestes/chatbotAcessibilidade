# ðŸ”¥ Guia de Testes de Carga e Performance

Este guia documenta como executar e interpretar testes de carga e performance do projeto.

---

## ðŸ“Š Testes de Carga com Locust

### O que Ã© Locust?

Locust Ã© uma ferramenta de teste de carga que simula mÃºltiplos usuÃ¡rios acessando a aplicaÃ§Ã£o simultaneamente. Permite identificar gargalos de performance e validar comportamento sob carga.

### CenÃ¡rios DisponÃ­veis

O arquivo `tests/load/locustfile.py` define 4 cenÃ¡rios:

1. **ChatbotUser** - Simula usuÃ¡rios fazendo perguntas (peso: chat=10, health=2, metrics=1)
2. **HealthCheckUser** - Simula monitoramento constante (apenas health checks)
3. **MixedUser** - Uso misto mais realista (chat=15, config=3, health=2, metrics=1)
4. **StressTestUser** - Teste de stress com requisiÃ§Ãµes rÃ¡pidas (0.1-0.5s entre requisiÃ§Ãµes)

### Como Executar

#### 1. Interface Web (Recomendado)

```bash
# Inicie o servidor em um terminal
uvicorn src.backend.api:app --host 0.0.0.0 --port 8000

# Em outro terminal, inicie o Locust
make test-load-ui
# ou
locust -f tests/load/locustfile.py --host=http://localhost:8000
```

Acesse http://localhost:8089 e configure:
- **Number of users**: 50-100 (usuÃ¡rios simultÃ¢neos)
- **Spawn rate**: 5-10 (usuÃ¡rios/segundo)
- **Host**: http://localhost:8000

#### 2. Modo Headless (CI/CD)

```bash
# Teste rÃ¡pido (50 usuÃ¡rios, 60 segundos)
make test-load

# Teste personalizado
locust -f tests/load/locustfile.py --host=http://localhost:8000 \
       --users 100 --spawn-rate 10 --run-time 120s --headless
```

### MÃ©tricas Importantes

- **RPS (Requests Per Second)**: Throughput da aplicaÃ§Ã£o
- **Response Time (p50, p95, p99)**: LatÃªncia em percentis
- **Failure Rate**: Taxa de erros
- **Users**: NÃºmero de usuÃ¡rios simultÃ¢neos

### Metas de Performance

| MÃ©trica | Meta | CrÃ­tico |
|:---|:---:|:---:|
| **Throughput** | > 50 req/s | > 100 req/s |
| **p95 Response Time** | < 2s | < 5s |
| **p99 Response Time** | < 5s | < 10s |
| **Failure Rate** | < 1% | < 5% |
| **Concurrent Users** | 50+ | 100+ |

---

## âš¡ Benchmarks de Performance com pytest-benchmark

### O que Ã© pytest-benchmark?

pytest-benchmark mede o tempo de execuÃ§Ã£o de funÃ§Ãµes especÃ­ficas, permitindo detectar regressÃµes de performance.

### Testes DisponÃ­veis

O arquivo `tests/performance/test_benchmarks.py` inclui:

1. **test_pipeline_performance** - Pipeline completo (meta: < 30s)
2. **test_cache_performance** - OperaÃ§Ãµes de cache (meta: < 10ms)
3. **test_validation_performance** - ValidaÃ§Ã£o de entrada (meta: < 1ms)
4. **test_formatter_performance** - FormataÃ§Ã£o de resposta (meta: < 100ms)
5. **test_metrics_collection_performance** - Coleta de mÃ©tricas (meta: < 1ms)
6. **test_concurrent_requests_performance** - 10 requisiÃ§Ãµes concorrentes (meta: < 5s)

### Como Executar

#### 1. Benchmarks Simples

```bash
# Executar todos os benchmarks
make test-benchmark

# Executar benchmark especÃ­fico
pytest tests/performance/test_benchmarks.py::test_cache_performance --benchmark-only -v
```

#### 2. Benchmarks com ComparaÃ§Ã£o

```bash
# Primeira execuÃ§Ã£o (salva baseline)
make test-benchmark-compare

# ExecuÃ§Ãµes futuras (compara com baseline)
make test-benchmark-compare
```

#### 3. Ver HistÃ³rico

```bash
# Listar execuÃ§Ãµes salvas
pytest-benchmark list

# Comparar duas execuÃ§Ãµes especÃ­ficas
pytest-benchmark compare 0001 0002
```

### Interpretando Resultados

```
Name (time in ms)                    Min      Max     Mean  StdDev  Median
test_cache_performance              5.23     8.91     6.12    0.89    5.98
test_validation_performance         0.45     1.23     0.67    0.21    0.61
```

- **Min/Max**: Tempo mÃ­nimo/mÃ¡ximo de execuÃ§Ã£o
- **Mean**: Tempo mÃ©dio
- **StdDev**: Desvio padrÃ£o (menor = mais consistente)
- **Median**: Mediana (menos afetada por outliers)

### Metas de Performance

| Teste | Meta | CrÃ­tico |
|:---|:---:|:---:|
| **Pipeline** | < 30s | < 60s |
| **Cache** | < 10ms | < 50ms |
| **Validation** | < 1ms | < 5ms |
| **Formatter** | < 100ms | < 500ms |
| **Metrics** | < 1ms | < 5ms |
| **Concurrent (10 req)** | < 5s | < 10s |

---

## ðŸŽ¯ Quando Executar

### Testes de Carga (Locust)

- âœ… Antes de deploy em produÃ§Ã£o
- âœ… ApÃ³s mudanÃ§as significativas na API
- âœ… Periodicamente (mensal) para validar escalabilidade
- âœ… Ao investigar problemas de performance

### Benchmarks (pytest-benchmark)

- âœ… Antes de cada commit (CI/CD)
- âœ… Ao otimizar cÃ³digo
- âœ… Para detectar regressÃµes de performance
- âœ… Ao comparar implementaÃ§Ãµes alternativas

---

## ðŸš¨ Troubleshooting

### Locust

**Problema**: "Connection refused"
- **SoluÃ§Ã£o**: Certifique-se de que o servidor estÃ¡ rodando em http://localhost:8000

**Problema**: Taxa de erro muito alta
- **SoluÃ§Ã£o**: Verifique logs do servidor, pode ser rate limiting ou timeout

**Problema**: Response time muito alto
- **SoluÃ§Ã£o**: Reduza nÃºmero de usuÃ¡rios ou aumente recursos do servidor

### pytest-benchmark

**Problema**: Resultados inconsistentes
- **SoluÃ§Ã£o**: Execute mÃºltiplas vezes, use `--benchmark-warmup`

**Problema**: Testes muito lentos
- **SoluÃ§Ã£o**: Use mocks para dependÃªncias externas (LLM, etc)

---

## ðŸ“ Boas PrÃ¡ticas

1. **Sempre execute testes de carga em ambiente isolado** (nÃ£o em produÃ§Ã£o)
2. **Use mocks para LLMs** em benchmarks (para resultados consistentes)
3. **Monitore recursos do sistema** (CPU, memÃ³ria, rede) durante testes de carga
4. **Salve baselines** de benchmarks para comparaÃ§Ã£o futura
5. **Documente mudanÃ§as** que causam regressÃµes de performance

---

## ðŸ”— Recursos Adicionais

- [DocumentaÃ§Ã£o Locust](https://docs.locust.io/)
- [DocumentaÃ§Ã£o pytest-benchmark](https://pytest-benchmark.readthedocs.io/)
- [Guia de Performance Testing](https://martinfowler.com/articles/practical-test-pyramid.html#PerformanceTests)

---

**Ãšltima atualizaÃ§Ã£o**: 2025-11-26
